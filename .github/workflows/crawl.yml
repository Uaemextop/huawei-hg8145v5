name: Web Crawler

on:
  workflow_dispatch:
    inputs:
      url:
        description: "URL to crawl (e.g. https://realfirmware.net)"
        required: true
        type: string
      depth:
        description: "Maximum crawl depth (0 = unlimited)"
        required: false
        default: "0"
        type: string
      delay:
        description: "Delay between requests in seconds"
        required: false
        default: "0.25"
        type: string
      no_robots:
        description: "Ignore robots.txt restrictions"
        required: false
        default: false
        type: boolean
      no_verify_ssl:
        description: "Disable TLS certificate verification"
        required: false
        default: false
        type: boolean
      force:
        description: "Re-download files even if they already exist on disk"
        required: false
        default: false
        type: boolean
      create_repo:
        description: "Create a new repository with the crawled site"
        required: false
        default: false
        type: boolean
      repo_owner:
        description: "User or organization for the new repository (required if create_repo is true)"
        required: false
        default: ""
        type: string
      git_push_every:
        description: "Commit and push crawled files every N saved files (0 = disabled, requires create_repo)"
        required: false
        default: "100"
        type: string
      skip_captcha:
        description: "Skip WAF/CAPTCHA detection â€“ save pages even if captcha signatures are found"
        required: false
        default: false
        type: boolean
      download_extensions:
        description: "File extensions to actively seek (comma-separated, e.g. zip,rar,bin,7z) or 'all' to download every file type"
        required: false
        default: "all"
        type: string
      concurrency:
        description: "Number of parallel download workers ('auto' = detect from CPU/RAM, or a number)"
        required: false
        default: "auto"
        type: string
      upload_extensions:
        description: "Extensions to upload to the crawl repo (comma-separated, e.g. zip,bin,rar) or 'all' to upload every file"
        required: false
        default: "all"
        type: string

permissions:
  contents: write

jobs:
  crawl:
    name: "Crawl ${{ inputs.url }}"
    runs-on: aapt

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Generate normalized repo name
        id: reponame
        run: |
          python3 -c "
          import re, sys
          url = sys.argv[1]
          # Remove scheme
          name = re.sub(r'^https?://', '', url)
          # Replace non-alphanumeric chars with hyphens
          name = re.sub(r'[^a-zA-Z0-9]+', '-', name)
          # Remove leading/trailing hyphens
          name = name.strip('-').lower()
          # Truncate to 100 chars max (GitHub limit)
          name = name[:100]
          print(f'name={name}')
          " "${{ inputs.url }}" >> "$GITHUB_OUTPUT"

      - name: Create repository for live progress
        if: inputs.create_repo == true
        id: create_repo
        env:
          ONT_TOKEN: ${{ secrets.ONT }}
        run: |
          set -euo pipefail

          REPO_BASE="${{ steps.reponame.outputs.name }}"
          OWNER="${{ inputs.repo_owner }}"

          if [ -z "$OWNER" ]; then
            RESP=$(curl -sS -w "\n%{http_code}" -H "Authorization: token $ONT_TOKEN" \
              https://api.github.com/user)
            HTTP_CODE=$(echo "$RESP" | tail -1)
            BODY=$(echo "$RESP" | sed '$d')
            if [ "$HTTP_CODE" != "200" ]; then
              echo "::error::Failed to get user info (HTTP $HTTP_CODE): $BODY"
              exit 1
            fi
            OWNER=$(echo "$BODY" | python3 -c "import sys,json; print(json.load(sys.stdin)['login'])")
          fi

          echo "Target owner: $OWNER"

          # Check if a repo already exists and find next available name
          REPO_NAME="$REPO_BASE"
          COPY=0
          MAX_COPIES=10
          while true; do
            HTTP_CODE=$(curl -sS -o /dev/null -w "%{http_code}" \
              -H "Authorization: token $ONT_TOKEN" \
              "https://api.github.com/repos/${OWNER}/${REPO_NAME}")
            if [ "$HTTP_CODE" = "404" ]; then
              break
            elif [ "$HTTP_CODE" != "200" ]; then
              echo "::error::Unexpected response (HTTP $HTTP_CODE) checking repo ${OWNER}/${REPO_NAME}. Verify your token and permissions."
              exit 1
            fi
            COPY=$((COPY + 1))
            if [ "$COPY" -gt "$MAX_COPIES" ]; then
              echo "::error::Exceeded maximum of $MAX_COPIES name attempts. Please delete old repos or choose a different name."
              exit 1
            fi
            REPO_NAME="${REPO_BASE}-${COPY}"
            echo "Repo exists, trying: $REPO_NAME"
          done

          echo "Creating repository: ${OWNER}/${REPO_NAME}"

          # Detect if owner is a user or an organization
          RESP=$(curl -sS -w "\n%{http_code}" -H "Authorization: token $ONT_TOKEN" \
            "https://api.github.com/users/${OWNER}")
          HTTP_CODE=$(echo "$RESP" | tail -1)
          BODY=$(echo "$RESP" | sed '$d')
          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::Failed to get owner info (HTTP $HTTP_CODE): $BODY"
            exit 1
          fi
          OWNER_TYPE=$(echo "$BODY" | python3 -c "import sys,json; print(json.load(sys.stdin).get('type','User'))")

          if [ "$OWNER_TYPE" = "Organization" ]; then
            API_URL="https://api.github.com/orgs/${OWNER}/repos"
          else
            API_URL="https://api.github.com/user/repos"
          fi

          RESP=$(curl -sS -w "\n%{http_code}" -X POST \
            -H "Authorization: token $ONT_TOKEN" \
            -H "Accept: application/vnd.github+json" \
            "$API_URL" \
            -d "{
              \"name\": \"${REPO_NAME}\",
              \"description\": \"Crawled site: ${{ inputs.url }}\",
              \"private\": false,
              \"auto_init\": false
            }")
          HTTP_CODE=$(echo "$RESP" | tail -1)
          BODY=$(echo "$RESP" | sed '$d')
          if [ "$HTTP_CODE" != "201" ]; then
            echo "::error::Failed to create repository (HTTP $HTTP_CODE): $BODY"
            exit 1
          fi
          echo "âœ… Repository created: ${OWNER}/${REPO_NAME}"

          # Initialize git in the output directory
          mkdir -p downloaded_site
          cd downloaded_site
          git init
          git config user.name "Web Crawler Bot"
          git config user.email "crawler@users.noreply.github.com"
          git config credential.helper '!f() { echo "password=$ONT_TOKEN"; }; f'
          git remote add origin "https://x-access-token@github.com/${OWNER}/${REPO_NAME}.git"
          git checkout -b main

          # Allow pushing large binary firmware files (up to 500 MB)
          git config http.postBuffer 524288000

          # Initial commit so push works
          cat > README.md <<EOF
          # Crawl of ${{ inputs.url }}

          - **Depth**: ${{ inputs.depth }}
          - **Delay**: ${{ inputs.delay }}s
          - **Download extensions**: ${{ inputs.download_extensions }}
          - **Upload extensions**: ${{ inputs.upload_extensions }}
          - **Concurrency**: ${{ inputs.concurrency }} workers
          - **Run**: #${{ github.run_number }}
          - **Status**: ðŸ”„ Crawling in progressâ€¦
          EOF
          git add -A
          git commit -m "Initial commit â€“ crawl starting"
          git push -u origin main

          echo "repo_ready=true" >> "$GITHUB_OUTPUT"
          echo "repo_full=${OWNER}/${REPO_NAME}" >> "$GITHUB_OUTPUT"

      - name: Run web crawler
        env:
          ONT_TOKEN: ${{ secrets.ONT }}
        run: |
          # Re-apply git credentials if repo was created
          if [ "${{ steps.create_repo.outputs.repo_ready }}" = "true" ]; then
            cd downloaded_site
            git config credential.helper '!f() { echo "password=$ONT_TOKEN"; }; f'
            cd ..
          fi

          GIT_PUSH_ARG=""
          if [ "${{ steps.create_repo.outputs.repo_ready }}" = "true" ] && [ "${{ inputs.git_push_every }}" != "0" ]; then
            GIT_PUSH_ARG="--git-push-every ${{ inputs.git_push_every }}"
          fi

          EXTRA_ARGS=""
          if [ "${{ inputs.no_robots }}" = "true" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --no-robots"
          fi
          if [ "${{ inputs.skip_captcha }}" = "true" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --no-check-captcha"
          fi
          if [ "${{ inputs.no_verify_ssl }}" = "true" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --no-verify-ssl"
          fi
          if [ "${{ inputs.force }}" = "true" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --force"
          fi

          # Upload-extension filtering: pass to crawler for periodic git push
          if [ "${{ inputs.upload_extensions }}" != "all" ] && [ -n "${{ inputs.upload_extensions }}" ]; then
            EXTRA_ARGS="$EXTRA_ARGS --upload-extensions ${{ inputs.upload_extensions }}"
            echo "ðŸ“¤ Upload filter: only ${{ inputs.upload_extensions }}"
          fi

          echo "ðŸ” Crawling ${{ inputs.url }} with depth=${{ inputs.depth }}, delay=${{ inputs.delay }}"
          echo "ðŸ“¦ Download extensions: ${{ inputs.download_extensions }}"

          python -m web_crawler "${{ inputs.url }}" \
            --depth "${{ inputs.depth }}" \
            --delay "${{ inputs.delay }}" \
            --output downloaded_site \
            --download-extensions "${{ inputs.download_extensions }}" \
            --concurrency "${{ inputs.concurrency }}" \
            $EXTRA_ARGS \
            $GIT_PUSH_ARG

      - name: Show download summary
        if: always()
        run: |
          echo "ðŸ“Š Download Summary"
          echo "==================="
          if [ -d downloaded_site ]; then
            echo "Total files: $(find downloaded_site -type f -not -path '*/\.git/*' -not -name '*.headers' | wc -l)"
            echo ""
            echo "Files by extension:"
            find downloaded_site -type f -not -path '*/\.git/*' -not -name '*.headers' | awk -F. '{if (NF>1) print $NF; else print "(no extension)"}' | sort | uniq -c | sort -rn | head -20
            echo ""
            echo "Binary/firmware files:"
            find downloaded_site -type f -not -path '*/\.git/*' \
              \( -name '*.rar' -o -name '*.bin' -o -name '*.7z' -o -name '*.tar' \
                 -o -name '*.gz' -o -name '*.exe' -o -name '*.bat' -o -name '*.cmd' \
                 -o -name '*.ps1' -o -name '*.sh' -o -name '*.zip' \) \
              -exec echo "  {}" \; 2>/dev/null || echo "  (none found)"
          else
            echo "No downloaded files found."
          fi

      - name: Final push of remaining files
        if: inputs.create_repo == true && steps.create_repo.outputs.repo_ready == 'true'
        env:
          ONT_TOKEN: ${{ secrets.ONT }}
        run: |
          cd downloaded_site
          git config credential.helper '!f() { echo "password=$ONT_TOKEN"; }; f'

          # Update README with completion status
          REPO_FULL="${{ steps.create_repo.outputs.repo_full }}"
          sed -i 's/ðŸ”„ Crawling in progressâ€¦/âœ… Crawl complete/' README.md
          git add README.md

          UPLOAD_EXTS="${{ inputs.upload_extensions }}"
          if [ "$UPLOAD_EXTS" != "all" ] && [ -n "$UPLOAD_EXTS" ]; then
            # Build a single find command with -o clauses for all extensions
            echo "ðŸ“¤ Filtering upload to extensions: $UPLOAD_EXTS"
            IFS=',' read -ra EXT_ARRAY <<< "$UPLOAD_EXTS"
            FIND_ARGS=()
            for ext in "${EXT_ARRAY[@]}"; do
              ext=$(echo "$ext" | xargs)          # trim whitespace
              ext="${ext#.}"                       # strip leading dot
              if [ ${#FIND_ARGS[@]} -gt 0 ]; then
                FIND_ARGS+=("-o")
              fi
              FIND_ARGS+=("-iname" "*.${ext}" "-o" "-iname" "*.${ext}.headers")
            done
            find . -not -path './.git/*' \( "${FIND_ARGS[@]}" \) -exec git add {} + 2>/dev/null || true
          else
            git add -A
          fi

          git diff --cached --quiet || git commit -m "Crawl complete: ${{ inputs.url }}"
          git push

      - name: Zip downloaded site
        run: |
          cd downloaded_site
          zip -r ../crawl-${{ steps.reponame.outputs.name }}.zip . -x '.git/*' -x '.git'

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: "crawl-${{ steps.reponame.outputs.name }}"
          path: downloaded_site/
          retention-days: 90

      - name: Create release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: "crawl-${{ steps.reponame.outputs.name }}-${{ github.run_number }}"
          name: "Crawl ${{ inputs.url }} (#${{ github.run_number }})"
          body: |
            ## Web Crawl Results

            - **URL**: ${{ inputs.url }}
            - **Depth**: ${{ inputs.depth }}
            - **Delay**: ${{ inputs.delay }}s
            - **robots.txt ignored**: ${{ inputs.no_robots }}
            - **SSL verification disabled**: ${{ inputs.no_verify_ssl }}
            - **Force re-download**: ${{ inputs.force }}
            - **Skip CAPTCHA check**: ${{ inputs.skip_captcha }}
            - **Download extensions**: ${{ inputs.download_extensions }}
            - **Upload extensions**: ${{ inputs.upload_extensions }}
            - **Concurrency**: ${{ inputs.concurrency }} workers
            - **Run**: #${{ github.run_number }}
            ${{ steps.create_repo.outputs.repo_ready == 'true' && format('- **Live repo**: https://github.com/{0}', steps.create_repo.outputs.repo_full) || '' }}
          files: "crawl-${{ steps.reponame.outputs.name }}.zip"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

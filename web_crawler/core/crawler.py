"""
Generic BFS web crawler.

Crawls a target website starting from a seed URL, downloading ALL reachable
pages and static assets with NO page limit.  Supports:

* robots.txt respect
* Configurable depth limit
* Resume from previously downloaded files
* Deduplication by content hash
* Soft-404 / false-positive detection
* WordPress auto-discovery (REST API, sitemaps, feeds, plugins, themes)
* User-Agent rotation and header-rotation retry on 403/402
* Cloudflare / WAF / CAPTCHA detection
* Exponential backoff on 429 (rate limiting)
* Saves ALL file types (html, php, asp, js, css, json, xml, txt, images, …)
* Saves HTTP response headers alongside each downloaded file
"""

import json
import random
import re
import string
import subprocess
import threading
import time
import urllib.parse
import urllib.robotparser
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import requests

try:
    from curl_cffi.requests.exceptions import RequestException as CfRequestException
except ImportError:
    CfRequestException = None  # type: ignore[misc,assignment]

# Network exception tuple that catches both requests and curl_cffi errors
_NETWORK_ERRORS: tuple[type[Exception], ...] = (requests.RequestException,)
if CfRequestException is not None:
    _NETWORK_ERRORS = (requests.RequestException, CfRequestException)

try:
    from tqdm import tqdm as _tqdm
    _TQDM_AVAILABLE = True
except ImportError:
    _TQDM_AVAILABLE = False

from web_crawler.config import (
    BACKOFF_429_BASE,
    BACKOFF_429_MAX,
    BLOCKED_PATH_RE,
    CRAWLABLE_TYPES,
    DEFAULT_DELAY,
    DEFAULT_CONCURRENCY,
    HEADER_RETRY_MAX,
    HIDDEN_FILE_PROBES,
    PROBE_403_THRESHOLD,
    PROBE_404_THRESHOLD,
    PROBE_DIR_404_LIMIT,
    REQUEST_TIMEOUT,
    RETRY_STATUS_CODES,
    SITEGROUND_BLOCKED_EXTENSIONS,
    SOFT_404_KEYWORDS,
    SOFT_404_MIN_KEYWORD_HITS,
    SOFT_404_SIZE_RATIO,
    SOFT_404_STANDALONE_MIN_HITS,
    SOFT_404_TITLE_KEYWORDS,
    USER_AGENTS,
    WAF_SIGNATURES,
    WP_DISCOVERY_PATHS,
    WP_PLUGIN_FILES,
    WP_PLUGIN_PROBES,
    WP_THEME_FILES,
    WP_THEME_PROBES,
    auto_concurrency,
)
from web_crawler.session import (
    build_cf_session, build_session, cache_bust_url, inject_cf_clearance,
    is_cf_managed_challenge, is_sg_captcha_response, random_headers,
    solve_cf_challenge, solve_sg_captcha,
)
from web_crawler.core.storage import content_hash, save_file, smart_local_path
from web_crawler.extraction.links import extract_links
from web_crawler.utils.log import ci_endgroup, ci_group, log
from web_crawler.utils.url import normalise_url, url_key, url_to_local_path


class Crawler:
    """
    Generic BFS web crawler.  Downloads every reachable page and asset
    from a target website with NO page limit.
    """

    def __init__(
        self,
        start_url: str,
        output_dir: Path,
        max_depth: int = 0,
        delay: float = DEFAULT_DELAY,
        verify_ssl: bool = True,
        respect_robots: bool = True,
        force: bool = False,
        git_push_every: int = 0,
        skip_captcha_check: bool = False,
        download_extensions: frozenset[str] | None = None,
        concurrency: int = DEFAULT_CONCURRENCY,
        upload_extensions: frozenset[str] | None = None,
        debug: bool = False,
        cf_clearance: str = "",
    ) -> None:
        parsed = urllib.parse.urlparse(start_url)
        self.start_url = start_url
        self.base = f"{parsed.scheme}://{parsed.netloc}"
        self.allowed_host = parsed.netloc
        self.output_dir = output_dir
        self.max_depth = max_depth
        self.delay = delay
        self.force = force
        self.debug = debug
        self.git_push_every = git_push_every
        self.skip_captcha_check = skip_captcha_check
        self.download_extensions = download_extensions or frozenset()
        self.upload_extensions = upload_extensions or frozenset()
        self.concurrency = auto_concurrency() if concurrency <= 0 else concurrency
        self._ext_link_re: re.Pattern | None = None
        if self.download_extensions:
            ext_pattern = "|".join(re.escape(e) for e in self.download_extensions)
            self._ext_link_re = re.compile(
                r'''(?:href|src|data-src|action)\s*=\s*['"]([^'"]*?(?:'''
                + ext_pattern
                + r""")(?:\?[^'"]*)?)['"]\s*""",
                re.I,
            )
        self.session = build_session(verify_ssl=verify_ssl)
        if cf_clearance:
            inject_cf_clearance(self.session, parsed.netloc, cf_clearance)
        self._lock = threading.Lock()     # protects shared state in concurrent mode

        self._visited: set[str] = set()
        self._queue: deque[tuple[str, int]] = deque()  # (url, depth)
        self._hashes: set[str] = set()
        self._probed_dirs: set[str] = set()  # directories already probed for hidden files
        self._probe_urls: set[str] = set()   # URLs generated by hidden-file / WP probing
        self._probe_403_count: int = 0       # consecutive 403s from probe URLs
        self._probe_404_count: int = 0       # consecutive 404s from probe URLs
        self._probe_dir_failures: dict[str, int] = {}  # per-directory probe failure count
        self._probing_disabled: bool = False  # set when threshold is reached
        self._sg_captcha_solves: int = 0     # how many inline captchas solved
        self._sg_solve_lock = threading.Lock()  # serialize concurrent CAPTCHA solves
        self._stats = {"ok": 0, "skip": 0, "err": 0, "dup": 0,
                       "soft404": 0, "waf": 0, "retry_ok": 0}

        # Soft-404 detection
        self._soft404_size: int | None = None
        self._soft404_hash: str | None = None

        # WordPress detection
        self._wp_detected: bool = False
        self._wp_probed: bool = False
        self._wp_confirmed_plugins: set[str] = set()
        self._wp_confirmed_themes: set[str] = set()

        # Cloudflare bypass state
        self._cf_bypass_done: bool = False

        # robots.txt (loaded after captcha solve in run())
        self._robots: urllib.robotparser.RobotFileParser | None = None
        self._respect_robots = respect_robots

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:
        ci_group("Crawl configuration")
        log.info("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        log.info("Output directory : %s", self.output_dir.resolve())
        log.info("Target URL       : %s", self.start_url)
        log.info("Allowed host     : %s", self.allowed_host)
        log.info("Page limit       : NONE (exhaustive)")
        if self.max_depth:
            log.info("Max depth        : %d", self.max_depth)
        if self.download_extensions:
            log.info("Seek extensions  : %s", ", ".join(sorted(self.download_extensions)))
        log.info("Concurrency      : %d workers", self.concurrency)
        if self.debug:
            log.info("Debug mode       : ON (headers saved, verbose logging)")
        if self.upload_extensions:
            log.info("Upload filter    : %s", ", ".join(sorted(self.upload_extensions)))
        log.info("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        ci_endgroup()

        # Pre-solve SiteGround CAPTCHA if the server uses it.
        # Must run before any other HTTP requests so the session cookie
        # is set for robots.txt loading and soft-404 probing.
        self._try_sg_captcha_bypass()

        # Check for Cloudflare Managed Challenge early, before wasting
        # time on robots.txt and soft-404 probing.
        self._check_cf_managed_challenge()

        # Load robots.txt (uses our session with the captcha cookie)
        if self._respect_robots:
            self._load_robots()

        # Build soft-404 baseline
        self._build_soft404_fingerprint()

        # Resume from disk
        if not self.force:
            n = self._resume_from_disk()
            if n:
                log.info("Resume: %d existing file(s) loaded from disk.", n)

        # Seed the queue
        self._enqueue(self.start_url, 0)

        log.info("Crawl started. Dynamic discovery begins.")

        if self.concurrency > 1:
            self._run_concurrent()
        elif _TQDM_AVAILABLE:
            self._run_with_progress()
        else:
            while self._queue:
                url, depth = self._queue.popleft()
                self._fetch_and_process(url, depth)

        self._log_final_stats()

    def _log_final_stats(self) -> None:
        """Print a summary table with final crawl statistics."""
        s = self._stats
        total = s["ok"] + s["skip"] + s["dup"] + s["err"] + s["soft404"] + s["waf"]
        pct_ok = (s["ok"] / total * 100) if total else 0
        pct_err = (s["err"] / total * 100) if total else 0
        ci_group("Crawl results")
        log.info("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        log.info("Crawl complete!")
        log.info("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        log.info("Visited     : %d URLs", len(self._visited))
        log.info("Saved (OK)  : %d  (%.1f%%)", s["ok"], pct_ok)
        log.info("Skipped     : %d", s["skip"])
        log.info("Duplicates  : %d", s["dup"])
        log.info("Soft-404    : %d", s["soft404"])
        log.info("WAF blocked : %d", s["waf"])
        log.info("Retry OK    : %d", s["retry_ok"])
        log.info("Errors      : %d  (%.1f%%)", s["err"], pct_err)
        log.info("Captcha     : %d solves", self._sg_captcha_solves)
        log.info("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        log.info("Files saved in: %s", self.output_dir.resolve())
        ci_endgroup()

    def _stats_postfix(self) -> dict[str, object]:
        """Return a dict suitable for tqdm ``set_postfix``."""
        return {
            "Q": len(self._queue),
            "OK": self._stats["ok"],
            "ERR": self._stats["err"],
            "SKIP": self._stats["skip"],
            "DUP": self._stats["dup"],
            "S404": self._stats["soft404"],
        }

    def _run_with_progress(self) -> None:
        """BFS loop with a tqdm progress bar."""
        bar = _tqdm(
            desc="Crawling",
            unit="URL",
            dynamic_ncols=True,
            bar_format=(
                "{l_bar}{bar}| {n_fmt}/{total_fmt} "
                "[{elapsed}<{remaining}, {rate_fmt}] {postfix}"
            ),
        )
        total_seen = len(self._queue) + len(self._visited)
        bar.total = total_seen

        while self._queue:
            url, depth = self._queue.popleft()
            prev_q = len(self._queue)
            self._fetch_and_process(url, depth)
            new_items = len(self._queue) - prev_q
            if new_items > 0:
                bar.total += new_items
                total_seen += new_items
            bar.update(1)
            bar.set_postfix(self._stats_postfix())

        bar.close()

    def _run_concurrent(self) -> None:
        """BFS loop with a ThreadPoolExecutor and live progress bar."""
        log.info("Concurrent mode: %d workers", self.concurrency)
        use_bar = _TQDM_AVAILABLE

        bar = None
        if use_bar:
            bar = _tqdm(
                desc="Crawling",
                unit="URL",
                dynamic_ncols=True,
                bar_format=(
                    "{l_bar}{bar}| {n_fmt}/{total_fmt} "
                    "[{elapsed}<{remaining}, {rate_fmt}] {postfix}"
                ),
                total=len(self._queue) + len(self._visited),
            )

        with ThreadPoolExecutor(max_workers=self.concurrency) as pool:
            while self._queue:
                # Drain up to `concurrency` items from the queue
                batch: list[tuple[str, int]] = []
                with self._lock:
                    while self._queue and len(batch) < self.concurrency:
                        batch.append(self._queue.popleft())
                if not batch:
                    break
                futures = {
                    pool.submit(self._fetch_and_process, url, depth): url
                    for url, depth in batch
                }
                for fut in as_completed(futures):
                    try:
                        fut.result()
                    except Exception as exc:
                        log.warning("Worker error for %s: %s",
                                    futures[fut], exc)
                    if bar is not None:
                        # Update progress after each completed future
                        with self._lock:
                            new_total = len(self._queue) + len(self._visited)
                        if new_total > bar.total:
                            bar.total = new_total
                        bar.update(1)
                        bar.set_postfix(self._stats_postfix())

        if bar is not None:
            bar.close()

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _load_robots(self) -> None:
        """Parse robots.txt from the target host."""
        robots_url = self.base + "/robots.txt"
        self._robots = urllib.robotparser.RobotFileParser()
        self._robots.set_url(robots_url)
        try:
            self._robots.read()
            log.info("Loaded robots.txt from %s", robots_url)
        except Exception as exc:
            log.debug("Could not load robots.txt: %s", exc)
            self._robots = None

    def _is_allowed(self, url: str) -> bool:
        """Check if the URL is allowed by robots.txt."""
        if self._robots is None:
            return True
        try:
            return self._robots.can_fetch("*", url)
        except Exception:
            return True

    def _try_sg_captcha_bypass(self) -> None:
        """If the target uses SiteGround CAPTCHA, solve the PoW
        challenge once so the session cookie is set."""
        log.info("Checking for SiteGround CAPTCHA …")
        solved = solve_sg_captcha(self.session, self.base, "/")
        if solved:
            # The server's meta-refresh suggests a 1-second wait
            # before the cookie is fully active.
            time.sleep(1)
            log.info("[SG-CAPTCHA] Solved – session cookie set")
        else:
            log.info("No SiteGround CAPTCHA detected (or not solvable)")

    def _check_cf_managed_challenge(self) -> None:
        """Detect and auto-solve a Cloudflare Managed Challenge.

        If the site returns ``cf-mitigated: challenge``, attempt to
        solve it by switching to a ``curl_cffi`` session (TLS fingerprint
        impersonation) or falling back to Playwright.
        """
        try:
            resp = self.session.get(
                self.start_url, timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
            )
        except _NETWORK_ERRORS:
            return
        if not is_cf_managed_challenge(resp):
            log.info("No Cloudflare challenge detected")
            return

        log.warning(
            "Cloudflare Managed Challenge detected on %s",
            self.start_url,
        )
        self._solve_cf_and_inject()

    def _solve_cf_and_inject(self) -> bool:
        """Switch the session to bypass Cloudflare.

        Uses ``curl_cffi`` TLS impersonation (preferred) or Playwright
        headless browser (fallback).  When ``curl_cffi`` works, the
        entire ``self.session`` is replaced with a ``curl_cffi`` session
        so that all subsequent requests use the same TLS fingerprint.

        Returns ``True`` on success.
        """
        cf_session = build_cf_session(verify_ssl=self.session.verify)
        if cf_session is not None:
            # Transfer existing cookies to the new session
            for cookie in self.session.cookies:
                cf_session.cookies.set(
                    cookie.name, cookie.value,
                    domain=cookie.domain, path=cookie.path,
                )
            try:
                check = cf_session.get(
                    self.start_url, timeout=REQUEST_TIMEOUT,
                    allow_redirects=True,
                )
                if check.ok and "just a moment" not in check.text[:2048].lower():
                    self.session = cf_session
                    log.info("[CF] Switched to curl_cffi session – bypass confirmed")
                    self._cf_bypass_done = True
                    return True
            except Exception as exc:
                log.debug("[CF] curl_cffi direct attempt failed: %s", exc)

        # Fallback: Playwright + cookie injection
        result = solve_cf_challenge(self.start_url)
        if not result:
            log.warning(
                "[CF] Could not auto-solve. Provide --cf-clearance <cookie> "
                "obtained from a browser session to bypass it.",
            )
            return False

        cookies, browser_ua = result
        parsed = urllib.parse.urlparse(self.start_url)
        for name, value in cookies.items():
            self.session.cookies.set(name, value,
                                     domain=parsed.netloc, path="/")
        self.session.headers["User-Agent"] = browser_ua
        log.info("[CF] %d cookies injected (UA synced) – verifying …",
                 len(cookies))
        try:
            check = self.session.get(
                self.start_url, timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
            )
            if check.ok and not is_cf_managed_challenge(check):
                log.info("[CF] Cloudflare bypass confirmed")
                self._cf_bypass_done = True
                return True
        except _NETWORK_ERRORS:
            pass
        log.warning("[CF] Cookies did not bypass the challenge")
        return False

    # ------------------------------------------------------------------
    # Soft-404 detection
    # ------------------------------------------------------------------

    def _build_soft404_fingerprint(self) -> None:
        """Fetch a random non-existent URL to fingerprint the server's
        custom error page (soft-404)."""
        slug = "".join(random.choices(string.ascii_lowercase, k=12))
        probe = f"{self.base}/_{slug}_does_not_exist_{slug}.html"
        try:
            resp = self.session.get(
                probe, timeout=REQUEST_TIMEOUT, allow_redirects=True
            )
        except _NETWORK_ERRORS:
            log.debug("Soft-404 probe failed (request error); detection disabled.")
            return

        if not resp.ok:
            # Server returns a real HTTP 404 – no soft-404 problem.
            log.debug("Server returns HTTP %s for missing pages – no soft-404.", resp.status_code)
            return

        # Server returned 200 for a non-existent page – soft-404 likely.
        body = resp.content
        self._soft404_size = len(body)
        self._soft404_hash = content_hash(body)
        log.info("Soft-404 baseline: %d bytes, hash=%s (server returns 200 for missing pages)",
            self._soft404_size, self._soft404_hash,
        )

    def _is_soft_404(self, content: bytes, url: str) -> bool:
        """Return True if *content* looks like a soft-404 (false positive).

        Detection layers:
        1. Exact hash match with the baseline probe.
        2. Size-based heuristic + keyword check (when baseline exists).
        3. ``<title>`` tag contains 404-related keywords.
        4. Standalone keyword check (works even without baseline).
        """
        text = content.decode("utf-8", errors="replace").lower()

        # --- Layer 1: baseline fingerprint exact match ---
        if self._soft404_hash is not None:
            if content_hash(content) == self._soft404_hash:
                log.debug("  Soft-404 (exact baseline match): %s", url)
                return True

            # --- Layer 2: size similarity + keywords ---
            size = len(content)
            if self._soft404_size and self._soft404_size > 0:
                ratio = abs(size - self._soft404_size) / self._soft404_size
                if ratio <= SOFT_404_SIZE_RATIO:
                    hits = sum(1 for kw in SOFT_404_KEYWORDS if kw in text)
                    if hits >= SOFT_404_MIN_KEYWORD_HITS:
                        log.debug(
                            "  Soft-404 (size+keywords, %d hits): %s",
                            hits, url,
                        )
                        return True

        # --- Layer 3: <title> tag contains 404-like keywords ---
        # Search only the first 4 KB where <title> typically appears.
        head = text[:4096]
        title_match = re.search(r"<title[^>]*>(.*?)</title>", head, re.S)
        if title_match:
            title = title_match.group(1).strip()
            for kw in SOFT_404_TITLE_KEYWORDS:
                if kw in title:
                    log.debug(
                        "  Soft-404 (title contains '%s'): %s", kw, url,
                    )
                    return True

        # --- Layer 4: standalone keyword check (no baseline needed) ---
        hits = sum(1 for kw in SOFT_404_KEYWORDS if kw in text)
        if hits >= SOFT_404_STANDALONE_MIN_HITS:
            log.debug(
                "  Soft-404 (standalone, %d keyword hits): %s", hits, url,
            )
            return True

        return False

    # ------------------------------------------------------------------
    # WordPress detection & discovery
    # ------------------------------------------------------------------

    @staticmethod
    def detect_wordpress(html: str) -> bool:
        """Return True if the HTML indicates a WordPress site."""
        lower = html.lower()
        indicators = [
            "wp-content/",
            "wp-includes/",
            'name="generator" content="wordpress',
            "/wp-json/",
            "wp-emoji-release.min.js",
        ]
        return any(ind in lower for ind in indicators)

    def _enqueue_wp_discovery(self, depth: int) -> None:
        """Enqueue WordPress-specific discovery URLs."""
        if self._wp_probed:
            return
        self._wp_probed = True
        for path in WP_DISCOVERY_PATHS:
            wp_url = self.base + path
            self._probe_urls.add(url_key(wp_url))
            self._enqueue(wp_url, 0)
        # Plugin enumeration (readme.txt to confirm existence)
        for slug in WP_PLUGIN_PROBES:
            wp_url = self.base + f"/wp-content/plugins/{slug}/readme.txt"
            self._probe_urls.add(url_key(wp_url))
            self._enqueue(wp_url, 0)
        # Theme enumeration (style.css to confirm existence)
        for slug in WP_THEME_PROBES:
            wp_url = self.base + f"/wp-content/themes/{slug}/style.css"
            self._probe_urls.add(url_key(wp_url))
            self._enqueue(wp_url, 0)
        # Author enumeration (users 1-10)
        for n in range(1, 11):
            wp_url = self.base + f"/?author={n}"
            self._probe_urls.add(url_key(wp_url))
            self._enqueue(wp_url, 0)
        total = (len(WP_DISCOVERY_PATHS) + len(WP_PLUGIN_PROBES)
                 + len(WP_THEME_PROBES) + 10)
        log.info("[WP] WordPress detected – enqueued %d discovery URLs", total)

        # Discover media file URLs (images, ZIPs) via REST API
        self._discover_wp_media()

    def _discover_wp_media(self) -> None:
        """Use the WP REST API to discover downloadable media files
        (images, ZIPs, etc.) and enqueue their direct URLs."""
        page = 1
        total = 0
        while page <= 20:  # safety cap
            api_url = (f"{self.base}/wp-json/wp/v2/media"
                       f"?per_page=100&page={page}")
            try:
                resp = self.session.get(api_url, timeout=REQUEST_TIMEOUT)
            except _NETWORK_ERRORS:
                break
            if resp.status_code != 200:
                break
            try:
                items = resp.json()
            except ValueError:
                break
            if not items:
                break
            for item in items:
                src = item.get("source_url", "")
                if src and self.allowed_host in src:
                    self._enqueue(src, 0, priority=True)
                    total += 1
            if len(items) < 100:
                break
            page += 1
            time.sleep(self.delay)
        if total:
            log.debug("  [WP-MEDIA] Discovered %d media files via REST API",
                     total)

    def _deep_crawl_wp_plugin(self, slug: str, depth: int) -> None:
        """Enqueue internal files for a confirmed WordPress plugin."""
        if slug in self._wp_confirmed_plugins:
            return
        self._wp_confirmed_plugins.add(slug)
        base_path = f"/wp-content/plugins/{slug}/"
        for f in WP_PLUGIN_FILES:
            self._enqueue(self.base + base_path + f, depth + 1)
        log.debug("  [WP-PLUGIN] Deep-crawling plugin '%s' (%d files)",
                 slug, len(WP_PLUGIN_FILES))

    def _deep_crawl_wp_theme(self, slug: str, depth: int) -> None:
        """Enqueue internal files for a confirmed WordPress theme."""
        if slug in self._wp_confirmed_themes:
            return
        self._wp_confirmed_themes.add(slug)
        base_path = f"/wp-content/themes/{slug}/"
        for f in WP_THEME_FILES:
            self._enqueue(self.base + base_path + f, depth + 1)
        log.debug("  [WP-THEME] Deep-crawling theme '%s' (%d files)",
                 slug, len(WP_THEME_FILES))

    # ------------------------------------------------------------------
    # WAF / Cloudflare / CAPTCHA detection
    # ------------------------------------------------------------------

    @staticmethod
    def detect_protection(headers: dict[str, str], body: str) -> list[str]:
        """Return a list of detected WAF/protection names from *headers* and
        *body* content.

        Only the first 8 KB of the body is inspected.  Real challenge /
        CAPTCHA pages are small and put indicators near the top, while
        large content pages may mention "captcha" or "cloudflare" deep
        in plugin configuration strings, causing false positives.
        """
        # Exclude Permissions-Policy – it merely declares allowed origins
        # (e.g. recaptcha.net, cloudflare.com) and is not a WAF indicator.
        filtered = {k: v for k, v in headers.items()
                    if k.lower() != "permissions-policy"}
        combined = " ".join(f"{k}: {v}" for k, v in filtered.items()).lower()
        combined += " " + body[:8192].lower()
        detected: list[str] = []
        for name, sigs in WAF_SIGNATURES.items():
            if any(s in combined for s in sigs):
                detected.append(name)
        return detected

    # ------------------------------------------------------------------
    # Header-rotation retry for 403 / 402
    # ------------------------------------------------------------------

    def _retry_with_headers(
        self, url: str
    ) -> requests.Response | None:
        """Retry *url* up to HEADER_RETRY_MAX times with different header
        profiles, cache-busted URLs, and Cloudflare-aware techniques.
        Returns a successful response or ``None``."""
        for attempt in range(1, HEADER_RETRY_MAX + 1):
            hdrs = random_headers(self.base)
            # Use cache-busted URL to bypass CDN/proxy cached 403
            bust_url = cache_bust_url(url)
            try:
                resp = self.session.get(
                    bust_url, timeout=REQUEST_TIMEOUT, allow_redirects=True,
                    headers=hdrs,
                )
                if resp.ok and not is_sg_captcha_response(resp):
                    log.debug(
                        "  [RETRY %d/%d] OK for %s (UA: %s…)",
                        attempt, HEADER_RETRY_MAX, url,
                        hdrs["User-Agent"][:40],
                    )
                    self._stats["retry_ok"] += 1
                    return resp
                # Cloudflare cookie-based challenge: first request sets
                # cf_clearance cookie, second request should succeed
                if not self._cf_bypass_done and resp.status_code == 403:
                    cf_cookies = {c.name for c in self.session.cookies
                                  if "cf" in c.name.lower()
                                  or "clearance" in c.name.lower()}
                    if cf_cookies:
                        self._cf_bypass_done = True
                        log.debug("  Cloudflare cookies found (%s), retrying",
                                  ", ".join(cf_cookies))
                        time.sleep(self.delay * 2)
                        resp2 = self.session.get(
                            url, timeout=REQUEST_TIMEOUT,
                            allow_redirects=True, headers=hdrs,
                        )
                        if resp2.ok:
                            self._stats["retry_ok"] += 1
                            log.debug("  [CF-BYPASS] Succeeded for %s", url)
                            return resp2
                log.debug(
                    "  [RETRY %d/%d] HTTP %s for %s",
                    attempt, HEADER_RETRY_MAX, resp.status_code, url,
                )
            except _NETWORK_ERRORS:
                pass
            time.sleep(self.delay * attempt)
        return None

    # ------------------------------------------------------------------
    # 429 exponential backoff
    # ------------------------------------------------------------------

    def _handle_rate_limit(self, resp: requests.Response, url: str) -> None:
        """Sleep with exponential backoff when a 429 is received."""
        retry_after = resp.headers.get("Retry-After")
        if retry_after and retry_after.isdigit():
            wait = min(int(retry_after), BACKOFF_429_MAX)
        else:
            wait = BACKOFF_429_BASE
        log.warning(
            "  [429] Rate limited on %s – sleeping %.1f s", url, wait
        )
        time.sleep(wait)

    _DISK_CT: dict[str, str] = {
        ".html": "text/html",
        ".htm":  "text/html",
        ".asp":  "text/html",
        ".aspx": "text/html",
        ".jsp":  "text/html",
        ".php":  "text/html",
        ".txt":  "text/plain",
        ".js":   "application/javascript",
        ".mjs":  "application/javascript",
        ".cjs":  "application/javascript",
        ".ts":   "application/javascript",
        ".jsx":  "application/javascript",
        ".tsx":  "application/javascript",
        ".css":  "text/css",
        ".scss": "text/css",
        ".sass": "text/css",
        ".less": "text/css",
        ".json": "application/json",
        ".xml":  "application/xml",
        ".svg":  "application/xml",
        ".rss":  "application/xml",
        ".atom": "application/xml",
        ".env":  "text/plain",
        ".cfg":  "text/plain",
        ".conf": "text/plain",
        ".config": "text/plain",
        ".hst":  "text/plain",
        ".ini":  "text/plain",
        ".toml": "text/plain",
        ".yml":  "text/plain",
        ".yaml": "text/plain",
        ".log":  "text/plain",
        ".sql":  "text/plain",
        ".csv":  "text/plain",
        ".tsv":  "text/plain",
        ".md":   "text/plain",
        ".rst":  "text/plain",
        ".py":   "text/plain",
        ".rb":   "text/plain",
        ".pl":   "text/plain",
        ".sh":   "text/plain",
        ".bat":  "text/plain",
        ".ps1":  "text/plain",
        ".lua":  "text/plain",
        ".go":   "text/plain",
        ".rs":   "text/plain",
        ".java": "text/plain",
        ".c":    "text/plain",
        ".cpp":  "text/plain",
        ".h":    "text/plain",
        ".vue":  "text/html",
        ".svelte": "text/html",
        ".htaccess": "text/plain",
        ".htpasswd": "text/plain",
        ".gitignore": "text/plain",
        ".dockerignore": "text/plain",
        ".editorconfig": "text/plain",
    }

    def _parse_local_file(self, local_path: Path, url: str) -> int:
        """Read a local file, extract links, and enqueue new ones."""
        ct = self._DISK_CT.get(local_path.suffix.lower())
        if ct is None:
            return 0
        try:
            content = local_path.read_bytes()
        except OSError as exc:
            log.debug("Could not read %s: %s", local_path, exc)
            return 0
        added = 0
        for link in extract_links(content, ct, url, self.base):
            k = url_key(link)
            if k not in self._visited:
                self._queue.append((link, 1))
                added += 1
        return added

    def _resume_from_disk(self) -> int:
        """Scan output_dir for previously downloaded files."""
        if not self.output_dir.exists():
            return 0
        count = 0
        for local_path in sorted(self.output_dir.rglob("*")):
            if not local_path.is_file():
                continue
            # Skip HTTP header files
            if local_path.suffix == ".headers":
                continue
            rel = local_path.relative_to(self.output_dir)
            path_str = "/" + str(rel).replace("\\", "/")
            if path_str == "/index.html":
                path_str = "/"
            elif path_str.endswith("/index.html"):
                path_str = path_str[:-len("index.html")]
            url = self.base + path_str
            key = url_key(url)
            if key in self._visited:
                continue
            self._visited.add(key)
            count += 1
            log.debug("Resume: %s → %s", local_path.name, url)
            self._parse_local_file(local_path, url)
        return count

    def _enqueue(self, url: str, depth: int, *, priority: bool = False) -> None:
        """Add *url* to the queue if not yet visited and within scope.

        When *priority* is ``True`` the URL is pushed to the front of
        the queue so that real-content pages are processed before
        speculative probe URLs.  URLs matching ``download_extensions``
        are always prioritized.
        """
        key = url_key(url)
        with self._lock:
            if key in self._visited:
                return
            # Only crawl URLs on the same host
            parsed = urllib.parse.urlparse(url)
            if parsed.netloc != self.allowed_host:
                return
            if self.max_depth and depth > self.max_depth:
                return
            # Auto-prioritize target extension files
            if not priority and self.download_extensions:
                path_lower = parsed.path.lower()
                if any(path_lower.endswith(ext) for ext in self.download_extensions):
                    priority = True
            if priority:
                self._queue.appendleft((url, depth))
            else:
                self._queue.append((url, depth))

    @staticmethod
    def _save_http_headers(local: Path, resp: requests.Response, url: str) -> None:
        """Save HTTP response headers as a .headers JSON file next to the content."""
        headers_path = local.parent / (local.name + ".headers")
        header_data = {
            "url": url,
            "status_code": resp.status_code,
            "headers": dict(resp.headers),
        }
        headers_path.parent.mkdir(parents=True, exist_ok=True)
        headers_path.write_text(
            json.dumps(header_data, indent=2, ensure_ascii=False),
            encoding="utf-8",
        )

    def _maybe_git_push(self) -> None:
        """Commit and push progress every *git_push_every* saved files.

        When *upload_extensions* is set, only files matching those
        extensions (plus README.md) are staged.  When debug mode is
        active, ``.headers`` files are included too.
        """
        if self.git_push_every <= 0:
            return
        if self._stats["ok"] % self.git_push_every != 0:
            return

        ok = self._stats["ok"]
        log.info("[GIT] Pushing progress (%d files saved so far)…", ok)
        try:
            cwd = str(self.output_dir.resolve())
            if self.upload_extensions:
                # Stage only files matching the upload extensions
                subprocess.run(
                    ["git", "add", "README.md"],
                    cwd=cwd, capture_output=True, timeout=30,
                )
                for ext in self.upload_extensions:
                    args = ["git", "add", "--", f"*{ext}"]
                    if self.debug:
                        args.append(f"*{ext}.headers")
                    subprocess.run(
                        args,
                        cwd=cwd, capture_output=True, timeout=60,
                    )
            else:
                subprocess.run(
                    ["git", "add", "-A"],
                    cwd=cwd, check=True, capture_output=True, timeout=60,
                )
            subprocess.run(
                ["git", "commit", "-m",
                 f"Crawl progress: {ok} files saved"],
                cwd=cwd, check=True, capture_output=True, timeout=60,
            )
            subprocess.run(
                ["git", "push"],
                cwd=cwd, check=True, capture_output=True, timeout=120,
            )
            log.info("[GIT] Push OK (%d files)", ok)
        except subprocess.CalledProcessError as exc:
            msg = exc.stderr.decode(errors="replace").strip() if exc.stderr else str(exc)
            log.warning("[GIT] Push failed: %s", msg)
        except FileNotFoundError:
            log.warning("[GIT] git not found – disabling periodic push")
            self.git_push_every = 0
        except Exception as exc:
            log.warning("[GIT] Push error: %s", exc)

    def _probe_hidden_files(self, url: str, depth: int) -> None:
        """Enqueue hidden/config files for every new directory discovered."""
        if self._probing_disabled:
            return

        parsed = urllib.parse.urlparse(url)
        path = parsed.path
        # Derive directory: strip filename if path doesn't end with /
        if path.endswith("/"):
            dir_path = path
        else:
            dir_path = path.rsplit("/", 1)[0] + "/"
        if not dir_path:
            dir_path = "/"

        with self._lock:
            if dir_path in self._probed_dirs:
                return
            self._probed_dirs.add(dir_path)

        for probe in HIDDEN_FILE_PROBES:
            probe_url = self.base + dir_path + probe
            with self._lock:
                self._probe_urls.add(url_key(probe_url))
            self._enqueue(probe_url, depth + 1)

        log.debug("Probed %d hidden files at %s", len(HIDDEN_FILE_PROBES), dir_path)

    def _fetch_and_process(self, url: str, depth: int) -> None:
        key = url_key(url)
        with self._lock:
            if key in self._visited:
                return
            self._visited.add(key)

        # Early skip for probe URLs whose directory already exhausted
        is_probe_early = key in self._probe_urls
        if is_probe_early:
            parsed_check = urllib.parse.urlparse(url)
            probe_dir = parsed_check.path.rsplit("/", 1)[0] + "/"
            with self._lock:
                dir_fails = self._probe_dir_failures.get(probe_dir, 0)
            if dir_fails >= PROBE_DIR_404_LIMIT:
                log.debug("[PROBE] Dir %s exhausted (%d fails) – skipping %s",
                          probe_dir, dir_fails, url)
                self._stats["err"] += 1
                return

        # Probe hidden/config files at each new directory
        self._probe_hidden_files(url, depth)

        # Blocked patterns
        parsed_url = urllib.parse.urlparse(url)
        if BLOCKED_PATH_RE.search(parsed_url.path):
            log.debug("Blocked URL, skipping: %s", url)
            return

        # Check robots.txt
        if not self._is_allowed(url):
            log.debug("Disallowed by robots.txt: %s", url)
            return

        local = smart_local_path(url, self.output_dir, "")

        # Skip already-downloaded files
        if not self.force and local.exists() and local.stat().st_size > 0:
            log.debug("[SKIP] Already on disk: %s", url)
            self._stats["skip"] += 1
            added = self._parse_local_file(local, url)
            if added:
                log.debug("  +%d new URLs from cached %s", added, local.name)
            return

        log.debug("[Q:%d OK:%d ERR:%d] GET %s",
                 len(self._queue), self._stats["ok"], self._stats["err"], url)

        # Rotate User-Agent per request (skip for curl_cffi which
        # manages its own UA via TLS impersonation)
        if not self._cf_bypass_done:
            self.session.headers["User-Agent"] = random.choice(USER_AGENTS)

        try:
            resp = self.session.get(
                url, timeout=REQUEST_TIMEOUT, allow_redirects=True
            )
        except _NETWORK_ERRORS as exc:
            log.warning("Request failed for %s – %s", url, exc)
            self._stats["err"] += 1
            return

        # Track final URL after redirects to avoid re-crawling
        final_url = resp.url
        if final_url != url:
            final_parsed = urllib.parse.urlparse(final_url)
            # Reject cross-domain redirects
            if final_parsed.netloc != self.allowed_host:
                log.debug("  Redirect to external host %s – skipping",
                          final_parsed.netloc)
                self._stats["err"] += 1
                return
            # WordPress protection: redirect to wp-login.php means
            # the page requires authentication – save the redirect
            # target but don't treat as error
            if "wp-login.php" in final_parsed.path:
                log.debug("  WP auth redirect to wp-login.php – skipping %s",
                          url)
                self._stats["err"] += 1
                return
            # Mark the final URL as visited too
            final_key = url_key(final_url)
            self._visited.add(final_key)
            log.debug("  Redirect: %s → %s", url, final_url)

        # Handle 429 (rate limiting) with exponential backoff + re-enqueue
        if resp.status_code == 429:
            self._handle_rate_limit(resp, url)
            self._visited.discard(key)
            self._enqueue(url, depth)
            return

        is_probe = key in self._probe_urls

        # Handle 403 / 402 – retry with rotated headers + cache busting
        if resp.status_code in RETRY_STATUS_CODES:
            # Cloudflare Managed Challenge – re-solve with Playwright
            if is_cf_managed_challenge(resp):
                log.info("[CF] Challenge on %s – re-solving …", url)
                if self._solve_cf_and_inject():
                    self._visited.discard(key)
                    self._enqueue(url, depth, priority=True)
                    return
            # Skip expensive retries for speculative probe URLs
            if is_probe:
                self._probe_403_count += 1
                # Track per-directory failures
                probe_parsed = urllib.parse.urlparse(url)
                probe_dir = probe_parsed.path.rsplit("/", 1)[0] + "/"
                with self._lock:
                    self._probe_dir_failures[probe_dir] = (
                        self._probe_dir_failures.get(probe_dir, 0) + 1
                    )
                if (self._probe_403_count >= PROBE_403_THRESHOLD
                        and not self._probing_disabled):
                    self._probing_disabled = True
                    log.info(
                        "[PROBE] %d consecutive 403s – disabling hidden-file "
                        "probing for remaining directories",
                        self._probe_403_count,
                    )
                log.debug("  [PROBE] 403 for %s – skipping (no retry)", url)
                self._stats["err"] += 1
                return

            log.debug("  [%d] Blocked – retrying with rotated headers: %s",
                     resp.status_code, url)
            retry_resp = self._retry_with_headers(url)
            if retry_resp is not None:
                resp = retry_resp
            else:
                # Try SG-Captcha solve as fallback for 403.
                # Serialize to avoid concurrent PoW solves.
                with self._sg_solve_lock:
                    if solve_sg_captcha(self.session, self.base,
                                        parsed_url.path):
                        try:
                            retry_resp = self.session.get(
                                url, timeout=REQUEST_TIMEOUT,
                                allow_redirects=True,
                            )
                        except _NETWORK_ERRORS:
                            retry_resp = None
                        if (retry_resp is not None and retry_resp.ok
                                and not is_sg_captcha_response(retry_resp)):
                            log.debug("  [SG-CAPTCHA] Solved 403 for %s", url)
                            resp = retry_resp
                        else:
                            self._stats["err"] += 1
                            return
                    else:
                        # Detect WAF / protection on the original blocked response
                        body_text = resp.content.decode("utf-8", errors="replace")
                        protections = self.detect_protection(
                            dict(resp.headers), body_text
                        )
                        if protections:
                            self._stats["waf"] += 1
                            log.warning(
                                "  [WAF] %s detected on %s",
                                ", ".join(protections), url,
                            )
                        else:
                            log.warning("HTTP %s for %s – skipping",
                                        resp.status_code, url)
                        self._stats["err"] += 1
                        return

        # Reset probe 403/404 streak on any successful probe response
        if is_probe and resp.ok:
            self._probe_403_count = 0
            self._probe_404_count = 0

        if not resp.ok:
            # For probe URLs returning 404, demote to debug and track
            # separately to avoid flooding logs with expected misses.
            if is_probe and resp.status_code == 404:
                self._probe_404_count += 1
                # Track per-directory failures so remaining probes for this
                # directory can be skipped without making HTTP requests.
                probe_parsed = urllib.parse.urlparse(url)
                probe_dir = probe_parsed.path.rsplit("/", 1)[0] + "/"
                with self._lock:
                    self._probe_dir_failures[probe_dir] = (
                        self._probe_dir_failures.get(probe_dir, 0) + 1
                    )
                if (self._probe_404_count >= PROBE_404_THRESHOLD
                        and not self._probing_disabled):
                    self._probing_disabled = True
                    log.info(
                        "[PROBE] %d consecutive 404s – disabling hidden-file "
                        "probing for remaining directories",
                        self._probe_404_count,
                    )
                log.debug("  [PROBE] 404 for %s – skipping", url)
                self._stats["err"] += 1
                return
            log.warning("HTTP %s for %s – skipping", resp.status_code, url)
            self._stats["err"] += 1
            return

        # SG-Captcha challenge: solve the PoW and retry the request
        # to get the real content behind the captcha.
        if is_sg_captcha_response(resp):
            # For probe URLs, treat SG-Captcha like a 403 – the file
            # almost certainly doesn't exist; solving PoW individually
            # for each probe is too slow.
            if is_probe:
                self._probe_403_count += 1
                # Track per-directory failures
                probe_parsed = urllib.parse.urlparse(url)
                probe_dir = probe_parsed.path.rsplit("/", 1)[0] + "/"
                with self._lock:
                    self._probe_dir_failures[probe_dir] = (
                        self._probe_dir_failures.get(probe_dir, 0) + 1
                    )
                log.debug("  [PROBE] SG-Captcha for %s – skipping", url)
                self._stats["err"] += 1
                return

            # Limit inline captcha solves to avoid endless PoW loops
            # when the server keeps re-challenging.
            if self._sg_captcha_solves >= PROBE_403_THRESHOLD:
                log.debug("  [SG-CAPTCHA] Solve limit reached – skipping %s",
                          url)
                self._stats["err"] += 1
                return

            # Serialize CAPTCHA solves: only one worker at a time.
            # When concurrent workers all hit the CAPTCHA, each would
            # independently spend ~20s solving PoW.  With the lock,
            # one worker solves while others wait, then re-try using
            # the shared bypass cookie.
            with self._sg_solve_lock:
                # Re-check: another worker may have solved while we waited
                try:
                    resp = self.session.get(
                        url, timeout=REQUEST_TIMEOUT, allow_redirects=True,
                    )
                except _NETWORK_ERRORS:
                    self._stats["err"] += 1
                    return
                if resp.ok and not is_sg_captcha_response(resp):
                    log.debug("  [SG-CAPTCHA] Bypassed via shared cookie: %s",
                              url)
                    # Fall through to normal processing below
                else:
                    log.debug("  [SG-CAPTCHA] Challenge for %s – solving …",
                              url)
                    solved = solve_sg_captcha(
                        self.session, self.base, parsed_url.path,
                    )
                    self._sg_captcha_solves += 1
                    if solved:
                        time.sleep(self.delay * 2)
                        try:
                            resp = self.session.get(
                                url, timeout=REQUEST_TIMEOUT,
                                allow_redirects=True,
                            )
                        except _NETWORK_ERRORS:
                            self._stats["err"] += 1
                            return
                        if not resp.ok or is_sg_captcha_response(resp):
                            log.warning(
                                "  [SG-CAPTCHA] Still blocked after solve: "
                                "HTTP %s for %s", resp.status_code, url)
                            self._stats["err"] += 1
                            return
                        log.debug("  [SG-CAPTCHA] Solved – got HTTP %s for %s",
                                  resp.status_code, url)
                        self._sg_captcha_solves = 0
                    else:
                        log.warning("  [SG-CAPTCHA] Failed to solve for %s",
                                    url)
                        self._stats["waf"] += 1
                        return

        content_type = resp.headers.get("Content-Type", "application/octet-stream")
        content_disp = resp.headers.get("Content-Disposition", "")
        content = resp.content
        ct_lower = content_type.split(";")[0].strip().lower()

        log.debug(
            "  ← HTTP %s  CT: %s  %d bytes",
            resp.status_code, content_type, len(content),
        )

        # Detect WAF / Cloudflare / CAPTCHA on successful responses too
        if ct_lower in ("text/html", "application/xhtml+xml"):
            text = content.decode("utf-8", errors="replace")
            if not self.skip_captcha_check:
                protections = self.detect_protection(dict(resp.headers), text)
                if protections:
                    log.warning("  [PROTECTION] %s on %s – not saving",
                                ", ".join(protections), url)
                    self._stats["waf"] += 1
                    return

            # Soft-404 detection – skip false positives
            if self._is_soft_404(content, url):
                self._stats["soft404"] += 1
                log.debug("  [SOFT-404] %s – not saving", url)
                return

            # WordPress detection on first HTML page
            if not self._wp_detected and self.detect_wordpress(text):
                self._wp_detected = True
                self._enqueue_wp_discovery(depth)

            # Extract WP nonce from HTML for REST API access
            if self._wp_detected:
                self._extract_wp_nonce(text)
        else:
            text = None

        # Deep-crawl confirmed WP plugins/themes
        path_lower = parsed_url.path.lower()
        if self._wp_detected and resp.ok:
            self._check_wp_deep_crawl(path_lower, depth)

        # Always save the file (every type)
        ch = content_hash(content)
        with self._lock:
            if ch in self._hashes:
                log.debug("  Duplicate content for %s – not saving again", url)
                self._stats["dup"] += 1
                is_dup = True
            else:
                self._hashes.add(ch)
                is_dup = False
        if not is_dup:
            local = smart_local_path(url, self.output_dir, content_type,
                                     content_disp)
            save_file(local, content)
            if self.debug:
                self._save_http_headers(local, resp, url)
            with self._lock:
                self._stats["ok"] += 1
            self._maybe_git_push()

        # Extract and enqueue links from parseable content
        ct = ct_lower
        is_parseable_ext = path_lower.endswith(
            (".asp", ".aspx", ".jsp", ".php", ".html", ".htm",
             ".js", ".mjs", ".cjs", ".ts", ".jsx", ".tsx",
             ".css", ".scss", ".sass", ".less",
             ".json", ".xml", ".svg", ".rss", ".atom",
             ".txt", ".csv", ".tsv", ".md", ".rst",
             ".env", ".cfg", ".conf", ".config", ".hst",
             ".ini", ".toml", ".yml", ".yaml",
             ".log", ".sql",
             ".py", ".rb", ".pl", ".sh", ".bat", ".ps1",
             ".lua", ".go", ".rs", ".java", ".c", ".cpp", ".h",
             ".vue", ".svelte",
             ".htaccess", ".htpasswd",
             ".gitignore", ".dockerignore", ".editorconfig")
        )
        if ct in CRAWLABLE_TYPES or is_parseable_ext:
            new_links = extract_links(content, content_type, url, self.base)
            # Also scan for links to target download extensions
            if self.download_extensions:
                new_links |= self._extract_extension_links(
                    content, url, self.download_extensions,
                )
            added = 0
            for link in new_links:
                k = url_key(link)
                if k not in self._visited:
                    self._enqueue(link, depth + 1, priority=True)
                    added += 1
            if added:
                log.debug("  +%d new URLs enqueued", added)

        time.sleep(self.delay)

    # ------------------------------------------------------------------
    # Extension-seeking link extraction
    # ------------------------------------------------------------------

    def _extract_extension_links(
        self, content: bytes, page_url: str, extensions: frozenset[str],
    ) -> set[str]:
        """Scan *content* for href/src attributes pointing to files
        with any of the target *extensions*.  Returns absolute URLs."""
        if self._ext_link_re is None:
            return set()
        text = content.decode("utf-8", errors="replace")
        found: set[str] = set()
        for m in self._ext_link_re.finditer(text):
            link = m.group(1).strip()
            if not link:
                continue
            link = urllib.parse.urljoin(page_url, link)
            found.add(link)
        return found

    # ------------------------------------------------------------------
    # WordPress deep-crawl helpers
    # ------------------------------------------------------------------

    _WP_NONCE_RE = re.compile(
        r"""(?:wp_rest_nonce|wpApiSettings[^}]*nonce)\W*[=:]\s*['"]([a-f0-9]+)['"]""",
        re.I,
    )

    def _extract_wp_nonce(self, html: str) -> None:
        """Extract a WP REST nonce from page HTML and set it on the session
        so subsequent REST API calls are authenticated."""
        m = self._WP_NONCE_RE.search(html)
        if m:
            nonce = m.group(1)
            self.session.headers["X-WP-Nonce"] = nonce
            log.debug("  WP nonce extracted: %s", nonce)

    def _check_wp_deep_crawl(self, path_lower: str, depth: int) -> None:
        """If a WP plugin/theme slug is confirmed (its readme.txt or
        style.css was fetched successfully), deep-crawl its internal
        files."""
        # Plugin: /wp-content/plugins/<slug>/readme.txt
        m = re.match(
            r"/wp-content/plugins/([a-z0-9_-]+)/readme\.txt$",
            path_lower,
        )
        if m:
            self._deep_crawl_wp_plugin(m.group(1), depth)
            return
        # Theme: /wp-content/themes/<slug>/style.css
        m = re.match(
            r"/wp-content/themes/([a-z0-9_-]+)/style\.css$",
            path_lower,
        )
        if m:
            self._deep_crawl_wp_theme(m.group(1), depth)
